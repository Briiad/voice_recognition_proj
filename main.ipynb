{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from micromlgen import port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "DATASET_PATH = \"mini_speech_commands\"\n",
    "SAMPLE_RATE = 22050\n",
    "JSON_PATH = \"data.json\"\n",
    "FRAMES = []\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "TEMPORARY_FILE = \"temp.wav\"\n",
    "RECORD_SECONDS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'mini_speech_commands\\down'\n",
      "\n",
      "Processing: 'mini_speech_commands\\stop'\n",
      "\n",
      "Processing: 'mini_speech_commands\\up'\n",
      "Finished processing.\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "def preprocess_dataset(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "  \n",
    "      # dictionary to store data\n",
    "      data = {\n",
    "          \"mapping\": [],\n",
    "          \"labels\": [],\n",
    "          \"mfcc\": [],\n",
    "          \"files\": []\n",
    "      }\n",
    "  \n",
    "      # loop through all sub-dirs\n",
    "      for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "  \n",
    "          # ensure we're not at root level\n",
    "          if dirpath is not dataset_path:\n",
    "  \n",
    "              # save label (i.e., sub-dir name) in the mapping\n",
    "              dirpath_components = dirpath.split(\"/\") # \"mini_speech_commands/down\" => [\"mini_speech_commands\", \"down\"]\n",
    "              semantic_label = dirpath_components[-1]\n",
    "              data[\"mapping\"].append(semantic_label)\n",
    "              print(\"\\nProcessing: '{}'\".format(semantic_label))\n",
    "  \n",
    "              # process files for a specific sub-dir\n",
    "              for f in filenames:\n",
    "  \n",
    "                  # load audio file\n",
    "                  file_path = os.path.join(dirpath, f)\n",
    "                  signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "                  if len(signal) >= SAMPLE_RATE: # ensure consistency of the length of the signal\n",
    "                    signal = signal[:SAMPLE_RATE]\n",
    "  \n",
    "                    # extract MFCCs\n",
    "                    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "  \n",
    "                    # store data for analysed track\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    data[\"labels\"].append(i-1)\n",
    "                    data[\"files\"].append(file_path)\n",
    "                    # print(\"{}: {}\".format(file_path, i-1))\n",
    "  \n",
    "      # save MFCCs to json file\n",
    "      with open(json_path, \"w\") as fp:\n",
    "          json.dump(data, fp, indent=4)\n",
    "  \n",
    "      print(\"Finished processing.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  preprocess_dataset(DATASET_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "  \n",
    "    # convert lists into numpy arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "  \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data using scikit learn\n",
    "def train_test_model(dataset_path):\n",
    "    # load data\n",
    "    X, y = load_data(dataset_path)\n",
    "  \n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # create network with linear regression\n",
    "    model = LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        multi_class='multinomial',\n",
    "        max_iter=100,\n",
    "        fit_intercept=True,\n",
    "        n_jobs=3,\n",
    "        C=0.1,\n",
    "        class_weight=None,\n",
    "        intercept_scaling=1,\n",
    "        penalty='l2',\n",
    "        random_state=None,\n",
    "        tol=0.0001,\n",
    "        verbose=0,\n",
    "        warm_start=False\n",
    "    )\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(512, 256),\n",
    "    #     activation=\"relu\",\n",
    "    #     solver=\"adam\",\n",
    "    #     batch_size=32,\n",
    "    #     verbose=1,\n",
    "    #     epsilon=1e-8,\n",
    "    #     alpha=0.0001,\n",
    "    #     learning_rate=\"adaptive\",\n",
    "    #     max_iter=100\n",
    "    # )\n",
    "\n",
    "    # reshape the 3d array to 2d array\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "    \n",
    "    # Print some details\n",
    "    print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "    print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "    print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "    print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "\n",
    "    # train network\n",
    "    # model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "  \n",
    "    # evaluate network\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # save the model using pickle\n",
    "    pickle.dump(model, open(\"model.sav\", 'wb'))\n",
    "\n",
    "    #save the model using json format\n",
    "    model_param = {}\n",
    "    model_param[\"coef_\"] = model.coef_.tolist()\n",
    "    model_param[\"intercept_\"] = model.intercept_.tolist()\n",
    "\n",
    "    json_txt = json.dumps(model_param, indent=4)\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(json_txt)\n",
    "    \n",
    "    # export to plain C\n",
    "    c_code = port(model, instance_name=\"MLClassifier\")\n",
    "\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "    \n",
    "    # print(c_code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "X_train.shape: (1877, 572)\n",
      "X_test.shape: (805, 572)\n",
      "y_train.shape: (1877,)\n",
      "y_test.shape: (805,)\n",
      "Accuracy: 72.67%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "if __name__ == \"__main__\":\n",
    "  print(\"Training model...\")\n",
    "  train_test_model(JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Audio and predict\n",
    "def record_audio():\n",
    "    # Record Audio\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        audio = r.listen(source)\n",
    "    \n",
    "    # Speech recognition using Google Speech Recognition\n",
    "    try:\n",
    "        # for testing purposes, we're just using the default API\n",
    "        # key to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "        # instead of `r.recognize_google(audio)`\n",
    "        print(\"You said: \" + r.recognize_google(audio))\n",
    "        return r.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio\")\n",
    "        return \"\"\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "        return \"\"\n",
    "      \n",
    "def predict_audio(audio):\n",
    "    # load the model\n",
    "    model = pickle.load(open(\"model.sav\", 'rb'))\n",
    "    # model = LogisticRegression(max_iter=200)\n",
    "    # model.coef_ = np.array(json.load(open(\"model.json\", 'r'))[\"coef_\"])\n",
    "    # model.intercept_ = np.array(json.load(open(\"model.json\", 'r'))[\"intercept_\"])\n",
    "\n",
    "    # extract MFCCs\n",
    "    signal, sample_rate = librosa.load(audio, sr=SAMPLE_RATE)\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfcc = mfcc.T\n",
    "\n",
    "    # predict\n",
    "    # mfcc = mfcc.reshape(1, mfcc.shape[0], mfcc.shape[1])\n",
    "    mfcc = mfcc.reshape(1, mfcc.shape[0] * mfcc.shape[1])\n",
    "    prediction = model.predict(mfcc)\n",
    "    print(\"Prediction: {}\".format(prediction))\n",
    "\n",
    "    # return the predicted class\n",
    "    return prediction[0]\n",
    "\n",
    "# Run the model\n",
    "if __name__ == \"__main__\": \n",
    "  # record audio\n",
    "  audio = record_audio()\n",
    "  # predict\n",
    "  predict_audio(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581d25aded393500ae6bebc8eb996d976b34c560b26d9468ea6715d44ce9c365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
