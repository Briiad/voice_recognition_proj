{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'mini_speech_commands\\down'\n",
      "\n",
      "Processing: 'mini_speech_commands\\stop'\n",
      "\n",
      "Processing: 'mini_speech_commands\\up'\n",
      "Finished processing.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"mini_speech_commands\"\n",
    "SAMPLE_RATE = 22050\n",
    "JSON_PATH = \"data.json\"\n",
    "\n",
    "# preprocess data\n",
    "def preprocess_dataset(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "  \n",
    "      # dictionary to store data\n",
    "      data = {\n",
    "          \"mapping\": [],\n",
    "          \"labels\": [],\n",
    "          \"mfcc\": [],\n",
    "          \"files\": []\n",
    "      }\n",
    "  \n",
    "      # loop through all sub-dirs\n",
    "      for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "  \n",
    "          # ensure we're not at root level\n",
    "          if dirpath is not dataset_path:\n",
    "  \n",
    "              # save label (i.e., sub-dir name) in the mapping\n",
    "              dirpath_components = dirpath.split(\"/\") # \"mini_speech_commands/down\" => [\"mini_speech_commands\", \"down\"]\n",
    "              semantic_label = dirpath_components[-1]\n",
    "              data[\"mapping\"].append(semantic_label)\n",
    "              print(\"\\nProcessing: '{}'\".format(semantic_label))\n",
    "  \n",
    "              # process files for a specific sub-dir\n",
    "              for f in filenames:\n",
    "  \n",
    "                  # load audio file\n",
    "                  file_path = os.path.join(dirpath, f)\n",
    "                  signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "                  if len(signal) >= SAMPLE_RATE: # ensure consistency of the length of the signal\n",
    "                    signal = signal[:SAMPLE_RATE]\n",
    "  \n",
    "                    # extract MFCCs\n",
    "                    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "  \n",
    "                    # store data for analysed track\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    data[\"labels\"].append(i-1)\n",
    "                    data[\"files\"].append(file_path)\n",
    "                    # print(\"{}: {}\".format(file_path, i-1))\n",
    "  \n",
    "      # save MFCCs to json file\n",
    "      with open(json_path, \"w\") as fp:\n",
    "          json.dump(data, fp, indent=4)\n",
    "  \n",
    "      print(\"Finished processing.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  preprocess_dataset(DATASET_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "  \n",
    "    # convert lists into numpy arrays\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "  \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data using scikit learn\n",
    "def train_test_model(dataset_path):\n",
    "    # load data\n",
    "    X, y = load_data(dataset_path)\n",
    "  \n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # create network\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(512, 256),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        epsilon=1e-8,\n",
    "        alpha=0.0001,\n",
    "        learning_rate=\"adaptive\",\n",
    "        max_iter=100\n",
    "    )\n",
    "\n",
    "    # reshape the 3d array to 2d array\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "    \n",
    "    # Print some details\n",
    "    print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "    print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "    print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "    print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "\n",
    "    # train network\n",
    "    # model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "  \n",
    "    # evaluate network\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # save the model using pickle\n",
    "    pickle.dump(model, open(\"model.sav\", 'wb'))\n",
    "  \n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "    # Save the model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "X_train.shape: (1877, 572)\n",
      "X_test.shape: (805, 572)\n",
      "y_train.shape: (1877,)\n",
      "y_test.shape: (805,)\n",
      "Iteration 1, loss = 12.07024103\n",
      "Iteration 2, loss = 3.51150639\n",
      "Iteration 3, loss = 2.66848892\n",
      "Iteration 4, loss = 2.03278477\n",
      "Iteration 5, loss = 1.86115911\n",
      "Iteration 6, loss = 1.11647758\n",
      "Iteration 7, loss = 0.93903701\n",
      "Iteration 8, loss = 0.55191036\n",
      "Iteration 9, loss = 0.31783776\n",
      "Iteration 10, loss = 0.35345651\n",
      "Iteration 11, loss = 0.38528393\n",
      "Iteration 12, loss = 0.45499704\n",
      "Iteration 13, loss = 0.28918085\n",
      "Iteration 14, loss = 0.24256560\n",
      "Iteration 15, loss = 0.23396956\n",
      "Iteration 16, loss = 0.19265405\n",
      "Iteration 17, loss = 0.15569327\n",
      "Iteration 18, loss = 0.10428395\n",
      "Iteration 19, loss = 0.14264763\n",
      "Iteration 20, loss = 0.26988042\n",
      "Iteration 21, loss = 0.36026191\n",
      "Iteration 22, loss = 0.45348788\n",
      "Iteration 23, loss = 0.27612799\n",
      "Iteration 24, loss = 0.21322782\n",
      "Iteration 25, loss = 0.28248682\n",
      "Iteration 26, loss = 0.17637604\n",
      "Iteration 27, loss = 0.20332008\n",
      "Iteration 28, loss = 0.21583508\n",
      "Iteration 29, loss = 0.14735169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 86.34%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "if __name__ == \"__main__\":\n",
    "  print(\"Training model...\")\n",
    "  train_test_model(JSON_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581d25aded393500ae6bebc8eb996d976b34c560b26d9468ea6715d44ce9c365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
